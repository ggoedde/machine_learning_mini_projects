{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transfer learning in TensorFlow using deep neural network.\n",
    "\n",
    "Train initial model on MNIST digits 0-4. Use pre-trained hidden\n",
    "layers to train new model on a very small subset of digits 5-9 images. \n",
    "Specifically, freeze the bottom layer from the 0-4 digit model and allow\n",
    "higher layers to train for 5-9 model.\n",
    "\n",
    "DNN includes the following:\n",
    "-5 layers\n",
    "-ELU activation function\n",
    "-He initialization\n",
    "-batch normalization\n",
    "-dropout\n",
    "\n",
    "Convolutional neural nets are better for image classification but purpose of \n",
    "this is to demonstrate transfer learning as well as above items that improve\n",
    "training of deep neural nets.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# model parameters\n",
    "N_INPUTS = 28 * 28\n",
    "N_NEURONS_BOTTOM = 200  # neurons in bottom layer\n",
    "N_NEURONS_OTHER = 50  # neurons in other layers\n",
    "N_OUTPUTS = 5  # digits 0-4 and 5-9\n",
    "KEEP_PROB = 0.5  # dropout rate\n",
    "BATCH_NORM_MOM = 0.9  # momentum for batch normalization\n",
    "# training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS_LOW = 25  # number of epochs to train 0-4\n",
    "N_EPOCHS_HIGH = 100  # number of epochs to train 5-9\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "SAVE_PATH_LOW = 'saved/low_digits/'\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class DeepNN(object):\n",
    "    \"\"\"Build deep neural network to classify MNIST digits.\"\"\"\n",
    "    def __init__(self, learning_rate, n_neurons_bottom, \n",
    "                 n_neurons_other, n_inputs, n_outputs, keep_prob):\n",
    "        self.lr = learning_rate\n",
    "        self.n_neurons_bottom = n_neurons_bottom\n",
    "        self.n_neurons_other = n_neurons_other\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        \"\"\"Create placeholders for inputs, outputs, and is_training.\n",
    "        is_training = True --> apply dropout during training\n",
    "        is_training = False --> don't apply dropout during testing\n",
    "        \"\"\"\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.n_inputs), \n",
    "                                    name='X')\n",
    "            self.y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "            self.is_training = tf.placeholder_with_default(False, \n",
    "                                                           shape=(), \n",
    "                                                           name='is_training')\n",
    "    \n",
    "    def _create_layer(self, prior_layer, n_neurons, name):\n",
    "        \"\"\"Create individual layer in neural network with he initialization, \n",
    "        batch normalization, and dropout.\n",
    "        \"\"\"\n",
    "        # He initialialization for variables\n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        # fully connected layer with he initialization\n",
    "        hidden = tf.layers.dense(prior_layer, \n",
    "                                 n_neurons, \n",
    "                                 kernel_initializer=he_init, \n",
    "                                 name=name)\n",
    "        # apply batch normalization\n",
    "        bn = tf.layers.batch_normalization(hidden, \n",
    "                                           training=self.is_training, \n",
    "                                           momentum=BATCH_NORM_MOM)\n",
    "        # use elu activation function\n",
    "        bn_act = tf.nn.elu(bn)\n",
    "        # apply dropout after batch normalization\n",
    "        hidden_drop = tf.layers.dropout(bn_act, \n",
    "                                        KEEP_PROB, \n",
    "                                        training=self.is_training)\n",
    "        return hidden_drop\n",
    "        \n",
    "    def _create_dnn(self):\n",
    "        \"\"\"Create deep neural network.\"\"\"\n",
    "        with tf.name_scope('dnn'):\n",
    "            # apply dropout to inputs\n",
    "            X_drop = tf.layers.dropout(self.X, self.keep_prob, \n",
    "                                       training=self.is_training)\n",
    "            # create hidden layers\n",
    "            hidden1 = self._create_layer(X_drop, self.n_neurons_bottom, 'hidden1')\n",
    "            hidden2 = self._create_layer(hidden1, self.n_neurons_other, 'hidden2')\n",
    "            hidden3 = self._create_layer(hidden2, self.n_neurons_other, 'hidden3')\n",
    "            hidden4 = self._create_layer(hidden3, self.n_neurons_other, 'hidden4')\n",
    "            # fully connected layer at end to compute outputs\n",
    "            self.logits = tf.layers.dense(hidden4, self.n_outputs, name='outputs')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\"Create cross entropy loss function.\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, \n",
    "                                                               logits=self.logits), \n",
    "                name='loss')\n",
    "            \n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Define optimizer to minimize loss.\"\"\"\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            # initially train_vars set equal to all trainable variables\n",
    "            # this is modified when model is restored\n",
    "            self.train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            # only train/update variables in train_vars\n",
    "            self.train_op = self.optimizer.minimize(self.loss, \n",
    "                                                    var_list=self.train_vars)\n",
    "            \n",
    "    def _create_eval(self):\n",
    "        \"\"\"Define loss evaluation metrics (accuracy).\"\"\"\n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(self.logits, self.y, 1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "    def build_model(self):\n",
    "        \"\"\"Build graph for deep neural network.\"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_dnn()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_eval()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "        \n",
    "def train_model(model, mnist, n_epochs, batch_size, save_path, restore=False):\n",
    "    \"\"\"Train model on MNIST data.\"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if restore:\n",
    "            # restore the bottom hidden layer from saved 0-4 digit model \n",
    "            # which will be freezed during training\n",
    "            reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \n",
    "                                           scope='hidden[1]')\n",
    "            reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "            restore_saver = tf.train.Saver(reuse_vars_dict) \n",
    "            \n",
    "            # initialize variables and restore model\n",
    "            sess.run(model.init)\n",
    "            restore_saver.restore(sess, SAVE_PATH_LOW)\n",
    "            \n",
    "            # we allow hidden layers 2-4 and the outputs to train\n",
    "            model.train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope='hidden[234]|outputs')\n",
    "            # redefine train_op so it knows which variables it can update\n",
    "            model.train_op = model.optimizer.minimize(model.loss, \n",
    "                                                      var_list=model.train_vars)\n",
    "        else:\n",
    "            # initialize variables for 0-4 digit model\n",
    "            sess.run(model.init)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(mnist.train.num_examples // batch_size):\n",
    "                # get random batch\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run(model.train_op, feed_dict={model.is_training: True,\n",
    "                                                    model.X: X_batch, \n",
    "                                                    model.y: y_batch})          \n",
    "            if epoch % 10 == 0:\n",
    "                # print test accuracy every 10 epochs\n",
    "                accuracy_val = sess.run(model.accuracy, \n",
    "                                        feed_dict={model.is_training: False,\n",
    "                                                   model.X: mnist.test.images,\n",
    "                                                   model.y: mnist.test.labels})\n",
    "                print('Epoch {0}, Test accuracy: {1:.3f}'.format(epoch, accuracy_val))\n",
    "        \n",
    "        print('\\nSaving model...')\n",
    "        if not restore and save_path: # save low digits model\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            saver.save(sess, save_path)\n",
    "\n",
    "            \n",
    "def mnist_subset_0_4(mnist, cutoff):\n",
    "    \"\"\"Create new MNIST object for low digits (0-4).\"\"\"\n",
    "    index_train = mnist.train.labels <= cutoff\n",
    "    index_test = mnist.test.labels <= cutoff\n",
    "        \n",
    "    # create new MNIST train dataset for digits 0-4\n",
    "    mnist_sub = copy.deepcopy(mnist)\n",
    "    mnist_sub.train._images = mnist.train.images[index_train == True]\n",
    "    mnist_sub.train._labels = mnist.train.labels[index_train == True]\n",
    "    mnist_sub.train._num_examples = len(mnist_sub.train.labels)\n",
    "    \n",
    "    # create new MNIST test dataset for digits 0-4\n",
    "    mnist_sub.test._images = mnist.test.images[index_test == True]\n",
    "    mnist_sub.test._labels = mnist.test.labels[index_test == True]\n",
    "    mnist_sub.test._num_examples = len(mnist_sub.test.labels)\n",
    "    \n",
    "    return mnist_sub\n",
    "\n",
    "\n",
    "def mnist_subset_5_9(mnist, cutoff, n_samples):\n",
    "    \"\"\"Create new MNIST object for high digits (5-9). \"\"\"\n",
    "    index_train = mnist.train.labels > cutoff\n",
    "    index_test = mnist.test.labels > cutoff\n",
    "        \n",
    "    # create small (n_samples) MNIST train dataset for digits 5-9 \n",
    "    mnist_sub = copy.deepcopy(mnist)\n",
    "    mnist_sub.train._images = mnist.train.images[index_train == True][:n_samples, :]\n",
    "    # shift labels to 0-4 as tf.nn.sparse_softmax_cross_entropy_with_logits requires\n",
    "    # them in this range. We retrain ouput layer so not an issue these labels are now\n",
    "    # the same as 0-4.\n",
    "    mnist_sub.train._labels = mnist.train.labels[index_train == True] - 5\n",
    "    mnist_sub.train._labels = mnist_sub.train.labels[:n_samples]\n",
    "    mnist_sub.train._num_examples = len(mnist_sub.train.labels)\n",
    "    \n",
    "    mnist_sub.test._images = mnist.test.images[index_test == True]\n",
    "    mnist_sub.test._labels = mnist.test.labels[index_test == True] - 5\n",
    "    mnist_sub.test._num_examples = len(mnist_sub.test.labels)\n",
    "    \n",
    "    return mnist_sub\n",
    "    \n",
    "    \n",
    "def split_data(mnist, cutoff):\n",
    "    \"\"\"Split datasets into MNIST digits 0-4 and MNIST digits 5-9.\"\"\"\n",
    "    # create MNIST dataset for images with labels 0-4\n",
    "    mnist_0_4 = mnist_subset_0_4(mnist, cutoff)\n",
    "    # create MNIST dataset for images with labels 5-9\n",
    "    # we only keep the first 500 images for training to demonstate\n",
    "    # transfer learning on a small dataset\n",
    "    mnist_5_9 = mnist_subset_5_9(mnist, cutoff, 500)\n",
    "    \n",
    "    return mnist_0_4, mnist_5_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# read MNIST dataset\n",
    "mnist_all = input_data.read_data_sets(DATA_PATH)\n",
    "# split into 0-4 and 5-9 digit datasets\n",
    "mnist_0_4, mnist_5_9 = split_data(mnist_all, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First train deep neural network on entire MNIST digits 0-4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test accuracy: 0.958\n",
      "Epoch 10, Test accuracy: 0.985\n",
      "Epoch 20, Test accuracy: 0.983\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# Create DeepNN model\n",
    "model = DeepNN(LEARNING_RATE, N_NEURONS_BOTTOM, \n",
    "               N_NEURONS_OTHER, N_INPUTS, N_OUTPUTS,  KEEP_PROB)\n",
    "model.build_model()\n",
    "# Train DNN on digist 0-4\n",
    "train_model(model, mnist_0_4, N_EPOCHS_LOW, \n",
    "            BATCH_SIZE, SAVE_PATH_LOW, restore=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model trained on digits 0-4 to train on digits 5-9. Rather than training on the entire 5-9 digit dataset, we select only 500 images (approx. 100 images per digit) to train on. This demonstrates pre-training on a semi-related large dataset, then actually training on the desired, yet much smaller dataset. For image classification tasks, deep neural networks tend to learn lower level features in the bottom layers and higher level features in the top layers. So we can expect the features learned in the bottom layer(s) for digits 0-4 to be similar to that for 5-9. We freeze the bottom layer from the 0-4 digit model while training the 5-9 digit model and allow the other layers to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved/low_digits/\n",
      "Epoch 0, Test accuracy: 0.561\n",
      "Epoch 10, Test accuracy: 0.784\n",
      "Epoch 20, Test accuracy: 0.837\n",
      "Epoch 30, Test accuracy: 0.832\n",
      "Epoch 40, Test accuracy: 0.873\n",
      "Epoch 50, Test accuracy: 0.847\n",
      "Epoch 60, Test accuracy: 0.853\n",
      "Epoch 70, Test accuracy: 0.839\n",
      "Epoch 80, Test accuracy: 0.837\n",
      "Epoch 90, Test accuracy: 0.851\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "train_model(model, mnist_5_9, N_EPOCHS_HIGH, BATCH_SIZE, save_path=None, restore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, train on the small 5-9 digits dataset without pre-training on 0-4 digit dataset. This achieves much lower test set accuracy than the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test accuracy: 0.574\n",
      "Epoch 10, Test accuracy: 0.684\n",
      "Epoch 20, Test accuracy: 0.712\n",
      "Epoch 30, Test accuracy: 0.701\n",
      "Epoch 40, Test accuracy: 0.732\n",
      "Epoch 50, Test accuracy: 0.706\n",
      "Epoch 60, Test accuracy: 0.709\n",
      "Epoch 70, Test accuracy: 0.717\n",
      "Epoch 80, Test accuracy: 0.712\n",
      "Epoch 90, Test accuracy: 0.723\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create DeepNN model\n",
    "model = DeepNN(LEARNING_RATE, N_NEURONS_BOTTOM, \n",
    "               N_NEURONS_OTHER, N_INPUTS, N_OUTPUTS,  KEEP_PROB)\n",
    "model.build_model()\n",
    "# Train DNN on digist 0-4\n",
    "train_model(model, mnist_5_9, N_EPOCHS_HIGH, BATCH_SIZE, save_path=None, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
